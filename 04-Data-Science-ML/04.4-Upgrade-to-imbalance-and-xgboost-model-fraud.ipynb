{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d3f5db0-8fe7-483f-8fc9-60bb9417f22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Let's improve and release a new model version in our repo using XGBoost\n",
    "\n",
    "As a Data Scientist, we realized that we could better detect fraud with XGboost if we add more custom logic to deal with imbalanced class.\n",
    "\n",
    "## Dealing with Imbalanced class\n",
    "\n",
    "Let's improve the generated notebook to better handle our imbalanced fraud dataset with 2 basic steps:\n",
    "\n",
    "* Add more weight to the classes relatively to their frequence (this will penalize more the model if it doesn't classify the fraud properly)\n",
    "* Switch the model to XGBoost\n",
    "\n",
    "*Note: Again, the goal of this notebook isn't to show a state of the art Fraud detection Model & imbalanced class. For more details on how to achieve that, please reach your Databricks Account team.*\n",
    "\n",
    "## Bootstraping our custom model using the last AutoML run\n",
    "\n",
    "To speedup things, we'll re-use our last AutoML run containing all best practices and use it to switch to XGBoost + change our class weights.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=984752964297111&notebook=%2F04-Data-Science-ML%2F04.4-Upgrade-to-imbalance-and-xgboost-model-fraud&demo_name=lakehouse-fsi-fraud&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-fsi-fraud%2F04-Data-Science-ML%2F04.4-Upgrade-to-imbalance-and-xgboost-model-fraud&version=1&user_hash=086247655aad7f847fc5af0bced92d31b6454844129a39a1b73eef221886867a\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee12bdc2-a649-420a-aa39-ba5c1a9a4ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.36.0 mlflow==2.19.0 hyperopt==0.2.7 shap==0.46.0\n",
    "# Hardcode dbrml 15.4 version here to avoid version conflict\n",
    "%pip install cloudpickle==2.2.1 databricks-automl-runtime==0.2.21 category-encoders==2.6.3 holidays==0.45 lightgbm==4.3.0 lightgbm==4.3.0 xgboost==2.1.3\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c7a401d-8120-4b16-85bf-2af3be1589dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c706aa26-e7fd-4f5e-a2c2-95054b05d499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import databricks.automl_runtime\n",
    "#Added for the demo purpose\n",
    "xp = DBDemos.get_last_experiment(\"lakehouse-fsi-fraud-detection\")\n",
    "\n",
    "# Use MLflow to track experiments\n",
    "mlflow.set_experiment(xp[\"path\"])\n",
    "\n",
    "#Run containing the data analysis notebook to get the data from artifact\n",
    "data_run = mlflow.search_runs(filter_string=\"tags.mlflow.source.name='Notebook: DataExploration'\").iloc[0].to_dict()\n",
    "\n",
    "#get best run id (this notebook)\n",
    "df = mlflow.search_runs(filter_string=\"metrics.val_f1_score < 1\")\n",
    "run = df.sort_values(by=\"metrics.val_f1_score\", ascending=False).iloc[0].to_dict()\n",
    "\n",
    "target_col = \"is_fraud\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd5afa1f-ed8f-41bf-85ce-ac2789601216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18581727-5502-441b-bba4-bc18f274de03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import uuid\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Create temp directory to download input data from MLflow\n",
    "input_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], \"tmp\", str(uuid.uuid4())[:8])\n",
    "os.makedirs(input_temp_dir)\n",
    "\n",
    "\n",
    "# Download the artifact and read it into a pandas DataFrame\n",
    "input_data_path = mlflow.artifacts.download_artifacts(run_id=data_run[\"run_id\"], artifact_path=\"data\", dst_path=input_temp_dir)\n",
    "\n",
    "df_loaded = pd.read_parquet(os.path.join(input_data_path, \"training_data\"))\n",
    "# Delete the temp data\n",
    "shutil.rmtree(input_temp_dir)\n",
    "try:\n",
    "    df_loaded = df_loaded.drop(['_automl_sample_weight_0000'], axis=1) #for demo only, to make it more stable across versions.\n",
    "except:\n",
    "    print('column weight not available - this might change depending on the automl version - can ignore')\n",
    "    \n",
    "# Preview data\n",
    "df_loaded.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fa530b-4b09-4b81-bbc6-6936e54a9fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Select supported columns\n",
    "Select only the columns that are supported. This allows us to train a model that can predict on a dataset that has extra columns that are not used in training.\n",
    "`[\"_rescued_data\", \"nameOrig\", \"id\", \"nameDest\", \"customer_id\"]` are dropped in the pipelines. See the Alerts tab of the AutoML Experiment page for details on why these columns are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b35ee16-7d48-4d98-9ac0-059282e296b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.automl_runtime.sklearn.column_selector import ColumnSelector\n",
    "supported_cols = [\"step\", \"countryLatOrig_lat\", \"diffDest\", \"oldBalanceOrig\", \"countryOrig_name\", \"age_group\", \"type\", \"countryLongOrig_long\", \"last_country_logged\", \"amount\", \"diffOrig\", \"countryOrig\", \"newBalanceDest\", \"oldBalanceDest\", \"newBalanceOrig\", \"countryLatDest_lat\", \"isUnauthorizedOverdraft\", \"countryDest\", \"countryDest_name\", \"country\", \"countryLongDest_long\"]\n",
    "col_selector = ColumnSelector(supported_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "189b7553-4109-4da5-9fe2-57a817d643f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c6d664c-cbe0-4751-a752-9680e7d420ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Boolean columns\n",
    "For each column, impute missing values and then convert into ones and zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ba7d191-d970-46ab-b180-12aedf019be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "\n",
    "\n",
    "bool_imputers = []\n",
    "\n",
    "bool_pipeline = Pipeline(steps=[\n",
    "    (\"cast_type\", FunctionTransformer(lambda df: df.astype(object))),\n",
    "    (\"imputers\", ColumnTransformer(bool_imputers, remainder=\"passthrough\")),\n",
    "    (\"onehot\", SklearnOneHotEncoder(handle_unknown=\"ignore\", drop=\"first\")),\n",
    "])\n",
    "\n",
    "bool_transformers = [(\"boolean\", bool_pipeline, [\"isUnauthorizedOverdraft\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "367abdfe-4dbc-4fe7-a3e9-8a84d961c5b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Numerical columns\n",
    "\n",
    "Missing values for numerical columns are imputed with mean by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ab4e40-3bea-471b-a608-0fbfc2dacefe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "\n",
    "num_imputers = []\n",
    "num_imputers.append((\"impute_mean\", SimpleImputer(), [\"age_group\", \"amount\", \"diffDest\", \"diffOrig\", \"isUnauthorizedOverdraft\", \"newBalanceDest\", \"newBalanceOrig\", \"oldBalanceDest\", \"oldBalanceOrig\", \"step\"]))\n",
    "\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    (\"converter\", FunctionTransformer(lambda df: df.apply(pd.to_numeric, errors=\"coerce\"))),\n",
    "    (\"imputers\", ColumnTransformer(num_imputers)),\n",
    "    (\"standardizer\", StandardScaler()),\n",
    "])\n",
    "\n",
    "numerical_transformers = [(\"numerical\", numerical_pipeline, [\"step\", \"isUnauthorizedOverdraft\", \"diffOrig\", \"newBalanceDest\", \"diffDest\", \"oldBalanceDest\", \"amount\", \"newBalanceOrig\", \"oldBalanceOrig\", \"age_group\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19681114-b458-4080-aef1-dbf1ff58aedf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cd67bae-24b6-4d45-ab1c-48e7ca25f1b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Low-cardinality categoricals\n",
    "Convert each low-cardinality categorical column into multiple binary columns through one-hot encoding.\n",
    "For each input categorical column (string or numeric), the number of output columns is equal to the number of unique values in the input column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf17798b-b6ca-4504-b1cc-50d074a7c83a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.automl_runtime.sklearn import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "one_hot_imputers = []\n",
    "\n",
    "one_hot_pipeline = Pipeline(steps=[\n",
    "    (\"imputers\", ColumnTransformer(one_hot_imputers, remainder=\"passthrough\")),\n",
    "    (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"indicator\")),\n",
    "])\n",
    "\n",
    "categorical_one_hot_transformers = [(\"onehot\", one_hot_pipeline, [\"age_group\", \"country\", \"countryDest\", \"countryDest_name\", \"countryLatDest_lat\", \"countryLatOrig_lat\", \"countryLongDest_long\", \"countryLongOrig_long\", \"countryOrig\", \"countryOrig_name\", \"last_country_logged\", \"type\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fceb1737-cb61-4f4e-9b6b-d75f23d612b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transformers = bool_transformers + numerical_transformers + categorical_one_hot_transformers\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers, remainder=\"passthrough\", sparse_threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "916165f5-40ae-4890-a593-9b7ce5316c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train - Validation - Test Split\n",
    "The input data is split by AutoML into 3 sets:\n",
    "- Train (60% of the dataset used to train the model)\n",
    "- Validation (20% of the dataset used to tune the hyperparameters of the model)\n",
    "- Test (20% of the dataset used to report the true performance of the model on an unseen dataset)\n",
    "\n",
    "`_automl_split_col_624d` contains the information of which set a given row belongs to.\n",
    "We use this column to split the dataset into the above 3 sets. \n",
    "The column should not be used for training so it is dropped after split is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edf9bd67-f811-4d56-b335-fa88eddef095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# AutoML completed train - validation - test split internally and used _automl_split_col_0000 to specify the set\n",
    "split_col = [c for c in df_loaded.columns if c.startswith('_automl_split_col')][0]\n",
    "\n",
    "split_train_df = df_loaded.loc[df_loaded[split_col] == \"train\"]\n",
    "split_val_df = df_loaded.loc[df_loaded[split_col].isin([\"val\", \"validate\"])]\n",
    "split_test_df = df_loaded.loc[df_loaded[split_col] == \"test\"]\n",
    "\n",
    "# Separate target column from features and drop _automl_split_col_0000\n",
    "X_train = split_train_df.drop([target_col, split_col], axis=1)\n",
    "y_train = split_train_df[target_col]\n",
    "\n",
    "X_val = split_val_df.drop([target_col, split_col], axis=1)\n",
    "y_val = split_val_df[target_col]\n",
    "\n",
    "X_test = split_test_df.drop([target_col, split_col], axis=1)\n",
    "y_test = split_test_df[target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1ea6c9-4f0a-45f5-913f-a14724df3eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Adding class weight to XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4d1e2cf-34a0-401f-9456-6c3e9bf20d80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "classes_weights = list(class_weight.compute_sample_weight(class_weight='balanced', y=y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a23dec-846a-40f8-a2bc-5e3de5d16184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train the new classification model: XGBoost\n",
    "\n",
    "- Log relevant metrics to MLflow to track runs\n",
    "- All the runs are logged under [this MLflow experiment](#mlflow/experiments/3254325001193021)\n",
    "- Change the model parameters and re-run the training cell to log a different trial to the MLflow experiment\n",
    "- To view the full list of tunable hyperparameters, check the output of the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f63bb1a5-de97-4b71-adc2-612941b5c883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "help(XGBClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c6d2fd1-86d7-40dd-846f-6ea4f80c623d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define the objective function\n",
    "The objective function used to find optimal hyperparameters. By default, this notebook only runs\n",
    "this function once (`max_evals=1` in the `hyperopt.fmin` invocation) with fixed hyperparameters, but\n",
    "hyperparameters can be tuned by modifying `space`, defined below. `hyperopt.fmin` will then use this\n",
    "function's return value to search the space to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49d37c84-1ed5-4eb7-9680-383fca744fe0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import Model, infer_signature, ModelSignature\n",
    "from mlflow.pyfunc import PyFuncModel\n",
    "from mlflow import pyfunc\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from databricks.automl_runtime.sklearn import TransformedTargetClassifier\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, STATUS_OK, Trials\n",
    "\n",
    "# Create a separate pipeline to transform the validation dataset. This is used for early stopping.\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "pipeline_val = Pipeline([\n",
    "    (\"column_selector\", col_selector),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "])\n",
    "pipeline_val.fit(X_train, y_train)\n",
    "X_val_processed = pipeline_val.transform(X_val)\n",
    "label_encoder_val = LabelEncoder()\n",
    "label_encoder_val.fit(y_train)\n",
    "y_val_processed = label_encoder_val.transform(y_val)\n",
    "\n",
    "def objective(params):\n",
    "  with mlflow.start_run(experiment_id=run['experiment_id']) as mlflow_run:\n",
    "    xgbc_classifier = TransformedTargetClassifier(\n",
    "        classifier=XGBClassifier(**params),\n",
    "        transformer=LabelEncoder()  # XGBClassifier requires the target values to be integers between 0 and n_class-1\n",
    "    )\n",
    "\n",
    "    model = Pipeline([\n",
    "        (\"column_selector\", col_selector),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", xgbc_classifier),\n",
    "    ])\n",
    "\n",
    "    # Enable automatic logging of input samples, metrics, parameters, and models\n",
    "    mlflow.sklearn.autolog(\n",
    "        log_input_examples=True,\n",
    "        silent=True)\n",
    "\n",
    "    model.fit(X_train, y_train, classifier__verbose=False, classifier__eval_set=[(X_val_processed,y_val_processed)], classifier__sample_weight=classes_weights)\n",
    "\n",
    "    \n",
    "    # Log metrics for the training set\n",
    "    mlflow_model = Model()\n",
    "    pyfunc.add_to_model(mlflow_model, loader_module=\"mlflow.sklearn\")\n",
    "    pyfunc_model = PyFuncModel(model_meta=mlflow_model, model_impl=model)\n",
    "    X_train[target_col] = y_train\n",
    "    training_eval_result = mlflow.evaluate(\n",
    "        model=pyfunc_model,\n",
    "        data=X_train,\n",
    "        targets=target_col,\n",
    "        model_type=\"classifier\",\n",
    "        evaluator_config = {\"log_model_explainability\": False,\n",
    "                            \"metric_prefix\": \"training_\" , \"pos_label\": 1 }\n",
    "    )\n",
    "    xgbc_training_metrics = training_eval_result.metrics\n",
    "    # Log metrics for the validation set\n",
    "    X_val[target_col] = y_val\n",
    "    val_eval_result = mlflow.evaluate(\n",
    "        model=pyfunc_model,\n",
    "        data=X_val,\n",
    "        targets=target_col,\n",
    "        model_type=\"classifier\",\n",
    "        evaluator_config = {\"log_model_explainability\": False,\n",
    "                            \"metric_prefix\": \"val_\" , \"pos_label\": 1 }\n",
    "    )\n",
    "    xgbc_val_metrics = val_eval_result.metrics\n",
    "    # Log metrics for the test set\n",
    "    X_test[target_col] = y_test\n",
    "    test_eval_result = mlflow.evaluate(\n",
    "        model=pyfunc_model,\n",
    "        data=X_test,\n",
    "        targets=target_col,\n",
    "        model_type=\"classifier\",\n",
    "        evaluator_config = {\"log_model_explainability\": False,\n",
    "                            \"metric_prefix\": \"test_\" , \"pos_label\": 1 }\n",
    "    )\n",
    "    xgbc_test_metrics = test_eval_result.metrics\n",
    "\n",
    "    loss = xgbc_val_metrics[\"val_f1_score\"]\n",
    "\n",
    "    # Truncate metric key names so they can be displayed together\n",
    "    xgbc_val_metrics = {k.replace(\"val_\", \"\"): v for k, v in xgbc_val_metrics.items()}\n",
    "    xgbc_test_metrics = {k.replace(\"test_\", \"\"): v for k, v in xgbc_test_metrics.items()}\n",
    "\n",
    "    return {\n",
    "      \"loss\": loss,\n",
    "      \"status\": STATUS_OK,\n",
    "      \"val_metrics\": xgbc_val_metrics,\n",
    "      \"test_metrics\": xgbc_test_metrics,\n",
    "      \"model\": model,\n",
    "      \"run\": mlflow_run,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118e9bce-ee26-4935-8f89-b21baa451172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure the hyperparameter search space\n",
    "Configure the search space of parameters. Parameters below are all constant expressions but can be\n",
    "modified to widen the search space. For example, when training a decision tree classifier, to allow\n",
    "the maximum tree depth to be either 2 or 3, set the key of 'max_depth' to\n",
    "`hp.choice('max_depth', [2, 3])`. Be sure to also increase `max_evals` in the `fmin` call below.\n",
    "\n",
    "See https://docs.databricks.com/applications/machine-learning/automl-hyperparam-tuning/index.html\n",
    "for more information on hyperparameter tuning as well as\n",
    "http://hyperopt.github.io/hyperopt/getting-started/search_spaces/ for documentation on supported\n",
    "search expressions.\n",
    "\n",
    "\n",
    "NOTE: The above URL points to a stable version of the documentation corresponding to the last\n",
    "released version of the package. The documentation may differ slightly for the package version\n",
    "used by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "331ecba9-8309-4f29-a1a7-d67e8965e02c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "space = {\n",
    "  \"colsample_bytree\": 0.22713530528852727,\n",
    "  \"learning_rate\": 0.03430054085463275,\n",
    "  \"max_depth\": 9,\n",
    "  \"min_child_weight\": 12,\n",
    "  \"n_estimators\": 3,\n",
    "  \"n_jobs\": 100,\n",
    "  \"subsample\": 0.5461546565369408,\n",
    "  \"verbosity\": 0,\n",
    "  \"random_state\": 441215911,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "514304a5-f91c-4b52-8353-9e6149c02f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run trials\n",
    "When widening the search space and training multiple models, switch to `SparkTrials` to parallelize\n",
    "training on Spark:\n",
    "```\n",
    "from hyperopt import SparkTrials\n",
    "trials = SparkTrials()\n",
    "```\n",
    "\n",
    "NOTE: While `Trials` starts an MLFlow run for each set of hyperparameters, `SparkTrials` only starts\n",
    "one top-level run; it will start a subrun for each set of hyperparameters.\n",
    "\n",
    "See http://hyperopt.github.io/hyperopt/scaleout/spark/ for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c018021-32ea-4fd3-9915-522180dc2726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "fmin(objective,\n",
    "     space=space,\n",
    "     algo=tpe.suggest,\n",
    "     max_evals=1,  # Increase this when widening the hyperparameter search space.\n",
    "     trials=trials)\n",
    "\n",
    "best_result = trials.best_trial[\"result\"]\n",
    "model = best_result[\"model\"]\n",
    "mlflow_run = best_result[\"run\"]\n",
    "\n",
    "display(\n",
    "  pd.DataFrame(\n",
    "    [best_result[\"val_metrics\"], best_result[\"test_metrics\"]],\n",
    "    index=[\"validation\", \"test\"]))\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71589aca-a518-4175-ac5c-001e418a3149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Feature importance\n",
    "\n",
    "SHAP is a game-theoretic approach to explain machine learning models, providing a summary plot\n",
    "of the relationship between features and model output. Features are ranked in descending order of\n",
    "importance, and impact/color describe the correlation between the feature and the target variable.\n",
    "- Generating SHAP feature importance is a very memory intensive operation, so to ensure that AutoML can run trials without\n",
    "  running out of memory, we disable SHAP by default.<br />\n",
    "  You can set the flag defined below to `shap_enabled = True` and re-run this notebook to see the SHAP plots.\n",
    "- To reduce the computational overhead of each trial, a single example is sampled from the validation set to explain.<br />\n",
    "  For more thorough results, increase the sample size of explanations, or provide your own examples to explain.\n",
    "- SHAP cannot explain models using data with nulls; if your dataset has any, both the background data and\n",
    "  examples to explain will be imputed using the mode (most frequent values). This affects the computed\n",
    "  SHAP values, as the imputed samples may not match the actual data distribution.\n",
    "\n",
    "For more information on how to read Shapley values, see the [SHAP documentation](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebda48a2-1759-43f0-9a38-c6347459627f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set this flag to True and re-run the notebook to see the SHAP plots\n",
    "shap_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c577aca-6362-46ae-88f7-8764aa878164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if shap_enabled:\n",
    "    from shap import KernelExplainer, summary_plot\n",
    "    # SHAP cannot explain models using data with nulls.\n",
    "    # To enable SHAP to succeed, both the background data and examples to explain are imputed with the mode (most frequent values).\n",
    "    mode = X_train.mode().iloc[0]\n",
    "\n",
    "    # Sample background data for SHAP Explainer. Increase the sample size to reduce variance.\n",
    "    train_sample = X_train.sample(n=min(100, X_train.shape[0]), random_state=441215911).fillna(mode)\n",
    "\n",
    "    # Sample some rows from the validation set to explain. Increase the sample size for more thorough results.\n",
    "    example = X_val.sample(n=min(100, X_val.shape[0]), random_state=441215911).fillna(mode)\n",
    "\n",
    "    # Use Kernel SHAP to explain feature importance on the sampled rows from the validation set.\n",
    "    predict = lambda x: model.predict(pd.DataFrame(x, columns=X_train.columns))\n",
    "    explainer = KernelExplainer(predict, train_sample, link=\"identity\")\n",
    "    shap_values = explainer.shap_values(example, l1_reg=False)\n",
    "    summary_plot(shap_values, example, class_names=model.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56dea96f-aa91-4fda-b9df-f9ffd2f495c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Confusion matrix, ROC and Precision-Recall curves for validation data\n",
    "\n",
    "We show the confusion matrix, ROC and Precision-Recall curves of the model on the validation data.\n",
    "\n",
    "For the plots evaluated on the training and the test data, check the artifacts on the MLflow run page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12748fc4-4f6f-4547-8fa5-bfa2a8f55493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run & click the link to see the MLflow run page\n",
    "displayHTML(f\"\"\"<a href=\"#mlflow/experiments/{ run['experiment_id'] }/runs/{ run['run_id'] }/artifactPath/model\">Link to model run page</a>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec49849-67c0-41b1-b8a1-50567083f8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from IPython.display import Image\n",
    "\n",
    "# Create temp directory to download MLflow model artifact\n",
    "eval_temp_dir = os.path.join(os.environ[\"SPARK_LOCAL_DIRS\"], \"tmp\", str(uuid.uuid4())[:8])\n",
    "os.makedirs(eval_temp_dir, exist_ok=True)\n",
    "\n",
    "# Download the artifact\n",
    "eval_path = mlflow.artifacts.download_artifacts(run_id=run['run_id'], dst_path=eval_temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbba7e95-6472-4ecd-9f3d-a435cb40d50b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Confusion matrix for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f4dc823-b6fb-49bf-b9a7-0d0da000cbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_confusion_matrix_path = os.path.join(eval_path, \"val_confusion_matrix.png\")\n",
    "display(Image(filename=eval_confusion_matrix_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8836db84-d3f8-4b36-ba40-d5e5353c1987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ROC curve for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76825c00-cc9c-420a-8c75-64f8a2d7beba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_roc_curve_path = os.path.join(eval_path, \"val_roc_curve_plot.png\")\n",
    "display(Image(filename=eval_roc_curve_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf41d5f-342e-4264-bd60-d42733d3e781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Precision-Recall curve for validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "694df33b-8096-4529-9111-cd0b8aae6e23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "eval_pr_curve_path = os.path.join(eval_path, \"val_precision_recall_curve_plot.png\")\n",
    "display(Image(filename=eval_pr_curve_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bdcbfbb-d2da-4bd9-9016-6ffb1155b7e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"dbdemos_fsi_fraud\"\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "#Use Databricks Unity Catalog to save our model\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "client = MlflowClient()\n",
    "\n",
    "#Fix pandas to a specific version to support all dbr version\n",
    "force_pandas_version(mlflow_run.info.run_id)\n",
    "\n",
    "#Add model within our catalog\n",
    "new_model = mlflow.register_model(f'runs:/{mlflow_run.info.run_id}/model', f\"{catalog}.{db}.{model_name}\")\n",
    "print(f\"Updating model to version {new_model.version}\")\n",
    "# Flag it as Production ready using UC Aliases\n",
    "client.set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"candidate\", version=new_model.version)\n",
    "DBDemos.set_model_permission(f\"{catalog}.{db}.{model_name}\", \"ALL_PRIVILEGES\", \"account users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "657d0188-08f0-47b6-b859-34ea267775aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Our new model is now ready !\n",
    "\n",
    "We're ready to deploy it in Production. However, as we're handling customer transactions in real-time, we want to make sure we start with a small, non disruptive release:\n",
    "\n",
    "* Without making any production outage\n",
    "* Route only a subset of the traffic to our new model\n",
    "* Measure model efficiency & ensure it's performing better.\n",
    "\n",
    "This is non-trivial and typically requests a full devops team. Thanksfully, Databricks makes this simple.\n",
    "\n",
    "Open the next notebook [04.5-AB-testing-model-serving-fraud]($./04.5-AB-testing-model-serving-fraud) to deploy our model and start routing a small subset of the requests to our new model!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04.4-Upgrade-to-imbalance-and-xgboost-model-fraud",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
